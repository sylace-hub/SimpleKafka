import java.io.{BufferedWriter, File, FileWriter, PrintWriter}
import java.nio.file.attribute.PosixFilePermissions
import java.nio.file.{Files, Paths}
import java.util.Collections
import java.util.Properties

import org.apache.kafka.clients.consumer.{ConsumerRecord, KafkaConsumer}
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import scalaj.http.{Http, HttpResponse}

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs._

import scala.collection.JavaConverters._
import com.typesafe.config.ConfigFactory

object Main {

  def main(args: Array[String]): Unit = {

    /* Producer */
    val producerProperties = new Properties()

    //producerProperties.put("bootstrap.servers", "hdfs://ip-10-0-0-218.eu-west-1.compute.internal:9092")
    producerProperties.put("bootstrap.servers", "http://127.0.0.1:9092")
    producerProperties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    producerProperties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

    val producer = new KafkaProducer[String, String](producerProperties)
    /* */

    // Topics :
    val topics = Array("USA", "FRA", "UK", "JAP")

    /* Consumer */
    val consumerProperties = new Properties()

    //consumerProperties.put("bootstrap.servers", "hdfs://ip-10-0-0-218.eu-west-1.compute.internal:9092")
    consumerProperties.put("bootstrap.servers", "http://127.0.0.1:9092")
    consumerProperties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    consumerProperties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    consumerProperties.put("group.id", "sylace-group")

    val consumer = new KafkaConsumer[String, String](consumerProperties)

    consumer.subscribe(topics.toList.asJava)
    /* */

    val requests: scala.collection.mutable.Map[String, HttpResponse[String]] = scala.collection.mutable.Map()

    val conf = new Configuration()
    conf.set("fs.DefaultFS", "hdfs://ip-10-0-0-218.eu-west-1.compute.internal:8020")
    conf.set("fs.hdfs.impl", classOf[org.apache.hadoop.hdfs.DistributedFileSystem].getName)
    conf.set("fs.file.impl", classOf[org.apache.hadoop.fs.LocalFileSystem].getName)
    val fs = FileSystem.get(conf)

    val outdir = ConfigFactory.load().getString("cluster-env.app.outdir")

    for (topic <- topics) {
      requests += (topic -> Http("https://newsapi.org/v2/everything").params(Map(
        "q" -> topic,
        "from" -> "2019-04-15",
        "to" -> "2019-04-18",
        "sortBy" -> "relevancy",
        "apiKey" -> "4e86ff31690e4f9b9b65c06ae033cc28"
      )).asString)

    }

    while (true) {
      println("Processing...")
      /* Writing */
      for ((k, v) <- requests) {
        val record = new ProducerRecord[String, String](k, "key", v.body)
        println("Sending record...")
        producer.send(record)
        println("Record sent")
      }
      /* */

      /* Reading */
      val records = consumer.poll(500)
      for (record: ConsumerRecord[String, String] <- records.asScala) {
        println()
        println("*** Consumer reading ***")
        println("----")
        println("Topic : " + record.topic())
        println("Value : " + record.value())
        val file = new File(outdir + "" + record.topic() + "/" + record.timestamp() + ".txt")
        file.setReadable(true, false)
        file.setWritable(true, false)
        val tempWriter = new PrintWriter(file)
        tempWriter.write(record.value())
        tempWriter.close()
        println()
      }
      /* */

      println("Sleeping a bit...")
      Thread.sleep(10000)

    }

    producer.close()
    consumer.close()
  }

}

